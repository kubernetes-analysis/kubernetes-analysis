#!/usr/bin/env python3
import argparse
import datetime
import json
import logging
import os
import time
from typing import Any, Optional, Tuple

from github import Github, Repository

import src.log as log
from src.data import API_DATA_JSON, Data

GITHUB_TOKEN = "GITHUB_TOKEN"
API_UPDATE_FILE = ".update"


def main():
    args = parse_args()
    log.setup(logging.INFO)

    if args.update_api:
        logging.info("Retrieving issues and PRs")
        token = get_github_token()
        github = Github(token)
        repo = github.get_repo("kubernetes/kubernetes")

        logging.info("Updating API")
        update_api(repo)

    elif args.update_bow:
        logging.info("Updating bag of words")
        update_bag_of_words()
    else:
        logging.info("Dumping all issues")
        dump_api(repo)


def parse_args() -> Any:
    parser = argparse.ArgumentParser()
    update_group = parser.add_mutually_exclusive_group()
    update_group.add_argument("--update-api",
                              "-u",
                              action="store_true",
                              help="Update the API dataset")
    update_group.add_argument("--update-bow",
                              "-b",
                              action="store_true",
                              help="Update the bag of words dataset")
    return parser.parse_args()


def get_github_token() -> Optional[str]:
    logging.info("Getting %s from environment variable", GITHUB_TOKEN)
    token = os.environ.get(GITHUB_TOKEN)
    if token is None:
        logging.critical("%s environment variable not set", GITHUB_TOKEN)
    return token


def dump_api(repo: Repository):
    result = []

    # We use the first (latest) issue as indicator of how many data we have to
    # fetch
    issues = repo.get_issues(state="all")
    latest_issue = issues[0]
    count = latest_issue.number
    logging.info("Pulling %d items", count)

    for i in range(1, count + 1):
        try:
            issue = repo.get_issue(i)
            logging.info("%u: %s", issue.number, issue.title)
            result.append(issue.raw_data)
        except Exception as err:
            logging.info("Unable to get data, waiting a minute: %s", err)
            time.sleep(60)

    with open(API_DATA_JSON, "w") as data_file:
        json.dump(result, data_file)

    logging.info("Done exporting %d items", i)
    Data.api_to_tarball()


def update_api(repo: Repository):
    (update_file, date) = get_update_file_date(API_UPDATE_FILE)

    json_list = []
    for issue in repo.get_issues(
            since=date,
            sort="updated",
            state="all",
    ):
        logging.info("%d: %s", issue.number, issue.title)
        json_list.append(issue.raw_data)

    data = Data()

    logging.info("Updating data")
    data.update_api_data(json_list)

    logging.info("Saving data")
    data.dump_api()

    Data.api_to_tarball()
    write_update_file_date(update_file)


def get_update_file_date(file_name: str) -> Tuple[Any, Any]:
    # load the update file contents
    update_file = open(file_name, "r+")
    date = datetime.datetime.strptime(update_file.read().strip(),
                                      "%Y-%m-%dT%H:%M:%S.%f")
    logging.info("Got update timestamp: %s", date.isoformat())
    return (update_file, date)


def write_update_file_date(update_file: Any):
    # update the file
    update_file.seek(0, 0)
    new_date = datetime.datetime.utcnow().isoformat()
    logging.info("New update timestamp: %s", new_date)
    update_file.write(new_date)


def update_bag_of_words():
    logging.info("Parsing data based on local dataset")
    data = Data(parse_nlp=True)

    logging.info("Saving data")
    data.dump_bag_of_words()


if __name__ == "__main__":
    main()
